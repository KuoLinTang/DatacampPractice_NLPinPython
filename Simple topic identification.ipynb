{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "generic-legislation",
   "metadata": {},
   "source": [
    "# Bag-of-words\n",
    "\n",
    "A basic technique to find topics in a text. <br/>\n",
    "重點：如果一個token的出現頻率很高，則此token對於整個text的重要性就提高。<br/>\n",
    "因此，bag-of-words可以用來根據words的頻率決定words在text中的重要性\n",
    "\n",
    "\n",
    "Procedures:\n",
    "1. Tokenize the text\n",
    "2. Count up all the tokens\n",
    "\n",
    "例如： <br/>\n",
    "Text: \"The cat is in the box. The cat likes the box. The box is over the cat.\"\n",
    "\n",
    "經過tokenization後可以得到以下詞類： <br/>\n",
    "\"The\", \"cat\", \"is\", \"in\", \"the\", \"box\", \"likes\", \"on\"\n",
    "\n",
    "這時，套用bag-of-words的概念來計算各個tokens的出現次數： <br/>\n",
    "\"The\":3 <br/>\n",
    "\"cat\":3 <br/>\n",
    "\"the\":3 <br/>\n",
    "\"box\":3 <br/>\n",
    "\"is\":2 <br/>\n",
    "\"in\":1 <br/>\n",
    "\"likes\":1 <br/>\n",
    "\"over\":1 <br/>\n",
    "\n",
    "這時會發現，\"The\"和\"the\"竟然是分開算的。由於兩個是同一個字，因此可以在bag-of-words之前加入一個步驟：把所有字詞轉換成小寫。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-spell",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bag-of-words in Python\n",
    "\n",
    "使用nltk的word_tokenize將字串或文件tokenize，再用collections這個module裡面的Counter函數來計算出現次數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expired-nelson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'is', 'in', 'the', 'box', '.', 'The', 'cat', 'likes', 'the', 'box', '.', 'The', 'box', 'is', 'over', 'the', 'cat', '.']\n",
      "Counter({'The': 3, 'cat': 3, 'the': 3, 'box': 3, '.': 3, 'is': 2, 'in': 1, 'likes': 1, 'over': 1})\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenization\n",
    "Text = \"The cat is in the box. The cat likes the box. The box is over the cat.\"\n",
    "tokens = word_tokenize(Text)\n",
    "print(tokens)\n",
    "\n",
    "# Count the tokens\n",
    "counter = Counter(tokens)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-crisis",
   "metadata": {},
   "source": [
    "上面的結果為一個Counter object，跟dictionaries的結構類似 <br/>\n",
    "可以從這個物件呼叫出前n個最頻繁出現的tokens (以tuple list的形式回傳)，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exposed-virginia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 3), ('cat', 3)]\n"
     ]
    }
   ],
   "source": [
    "# 設定n = 2\n",
    "print(counter.most_common(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-photography",
   "metadata": {},
   "source": [
    "注意：出現頻率為3的tokens應該有4個，不過這裡只出現兩個，是因為most_common不會sort tokens也不會告訴使用者還有更多相同頻率的tokens，代表用此方法容易造成忽略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-clearance",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "課堂練習一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collected-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "painful-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = '\\'\\'\\'Debugging\\'\\'\\' is the process of finding and resolving of defects that prevent correct operation of computer software or a system.  \\n\\nNumerous books have been written about debugging (see below: #Further reading|Further reading), as it involves numerous aspects, including interactive debugging, control flow, integration testing, Logfile|log files, monitoring (Application monitoring|application, System Monitoring|system), memory dumps, Profiling (computer programming)|profiling, Statistical Process Control, and special design tactics to improve detection while simplifying changes.\\n\\nOrigin\\nA computer log entry from the Mark&nbsp;II, with a moth taped to the page\\n\\nThe terms \"bug\" and \"debugging\" are popularly attributed to Admiral Grace Hopper in the 1940s.[http://foldoc.org/Grace+Hopper Grace Hopper]  from FOLDOC While she was working on a Harvard Mark II|Mark II Computer at Harvard University, her associates discovered a moth stuck in a relay and thereby impeding operation, whereupon she remarked that they were \"debugging\" the system. However the term \"bug\" in the meaning of technical error dates back at least to 1878 and Thomas Edison (see software bug for a full discussion), and \"debugging\" seems to have been used as a term in aeronautics before entering the world of computers. Indeed, in an interview Grace Hopper remarked that she was not coining the term{{Citation needed|date=July 2015}}. The moth fit the already existing terminology, so it was saved.  A letter from J. Robert Oppenheimer (director of the WWII atomic bomb \"Manhattan\" project at Los Alamos, NM) used the term in a letter to Dr. Ernest Lawrence at UC Berkeley, dated October 27, 1944,http://bancroft.berkeley.edu/Exhibits/physics/images/bigscience25.jpg regarding the recruitment of additional technical staff.\\n\\nThe Oxford English Dictionary entry for \"debug\" quotes the term \"debugging\" used in reference to airplane engine testing in a 1945 article in the Journal of the Royal Aeronautical Society. An article in \"Airforce\" (June 1945 p.&nbsp;50) also refers to debugging, this time of aircraft cameras.  Hopper\\'s computer bug|bug was found on September 9, 1947. The term was not adopted by computer programmers until the early 1950s.\\nThe seminal article by GillS. Gill, [http://www.jstor.org/stable/98663 The Diagnosis of Mistakes in Programmes on the EDSAC], Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, Vol. 206, No. 1087 (May 22, 1951), pp. 538-554 in 1951 is the earliest in-depth discussion of programming errors, but it does not use the term \"bug\" or \"debugging\".\\nIn the Association for Computing Machinery|ACM\\'s digital library, the term \"debugging\" is first used in three papers from 1952 ACM National Meetings.Robert V. D. Campbell, [http://portal.acm.org/citation.cfm?id=609784.609786 Evolution of automatic computation], Proceedings of the 1952 ACM national meeting (Pittsburgh), p 29-32, 1952.Alex Orden, [http://portal.acm.org/citation.cfm?id=609784.609793 Solution of systems of linear inequalities on a digital computer], Proceedings of the 1952 ACM national meeting (Pittsburgh), p. 91-95, 1952.Howard B. Demuth, John B. Jackson, Edmund Klein, N. Metropolis, Walter Orvedahl, James H. Richardson, [http://portal.acm.org/citation.cfm?id=800259.808982 MANIAC], Proceedings of the 1952 ACM national meeting (Toronto), p. 13-16 Two of the three use the term in quotation marks.\\nBy 1963 \"debugging\" was a common enough term to be mentioned in passing without explanation on page 1 of the Compatible Time-Sharing System|CTSS manual.[http://www.bitsavers.org/pdf/mit/ctss/CTSS_ProgrammersGuide.pdf The Compatible Time-Sharing System], M.I.T. Press, 1963\\n\\nKidwell\\'s article \\'\\'Stalking the Elusive Computer Bug\\'\\'Peggy Aldrich Kidwell, [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?tp=&arnumber=728224&isnumber=15706 Stalking the Elusive Computer Bug], IEEE Annals of the History of Computing, 1998. discusses the etymology of \"bug\" and \"debug\" in greater detail.\\n\\nScope\\nAs software and electronic systems have become generally more complex, the various common debugging techniques have expanded with more methods to detect anomalies, assess impact, and schedule software patches or full updates to a system. The words \"anomaly\" and \"discrepancy\" can be used, as being more neutral terms, to avoid the words \"error\" and \"defect\" or \"bug\" where there might be an implication that all so-called \\'\\'errors\\'\\', \\'\\'defects\\'\\' or \\'\\'bugs\\'\\' must be fixed (at all costs). Instead, an impact assessment can be made to determine if changes to remove an \\'\\'anomaly\\'\\' (or \\'\\'discrepancy\\'\\') would be cost-effective for the system, or perhaps a scheduled new release might render the change(s) unnecessary. Not all issues are life-critical or mission-critical in a system. Also, it is important to avoid the situation where a change might be more upsetting to users, long-term, than living with the known problem(s) (where the \"cure would be worse than the disease\"). Basing decisions of the acceptability of some anomalies can avoid a culture of a \"zero-defects\" mandate, where people might be tempted to deny the existence of problems so that the result would appear as zero \\'\\'defects\\'\\'. Considering the collateral issues, such as the cost-versus-benefit impact assessment, then broader debugging techniques will expand to determine the frequency of anomalies (how often the same \"bugs\" occur) to help assess their impact to the overall system.\\n\\nTools\\nDebugging on video game consoles is usually done with special hardware such as this Xbox (console)|Xbox debug unit intended for developers.\\n\\nDebugging ranges in complexity from fixing simple errors to performing lengthy and tiresome tasks of data collection, analysis, and scheduling updates.  The debugging skill of the programmer can be a major factor in the ability to debug a problem, but the difficulty of software debugging varies greatly with the complexity of the system, and also depends, to some extent, on the programming language(s) used and the available tools, such as \\'\\'debuggers\\'\\'. Debuggers are software tools which enable the programmer to monitor the execution (computers)|execution of a program, stop it, restart it, set breakpoints, and change values in memory. The term \\'\\'debugger\\'\\' can also refer to the person who is doing the debugging.\\n\\nGenerally, high-level programming languages, such as Java (programming language)|Java, make debugging easier, because they have features such as exception handling that make real sources of erratic behaviour easier to spot. In programming languages such as C (programming language)|C or assembly language|assembly, bugs may cause silent problems such as memory corruption, and it is often difficult to see where the initial problem happened. In those cases, memory debugging|memory debugger tools may be needed.\\n\\nIn certain situations, general purpose software tools that are language specific in nature can be very useful.  These take the form of \\'\\'List of tools for static code analysis|static code analysis tools\\'\\'.  These tools look for a very specific set of known problems, some common and some rare, within the source code.  All such issues detected by these tools would rarely be picked up by a compiler or interpreter, thus they are not syntax checkers, but more semantic checkers.  Some tools claim to be able to detect 300+ unique problems. Both commercial and free tools exist in various languages.  These tools can be extremely useful when checking very large source trees, where it is impractical to do code walkthroughs.  A typical example of a problem detected would be a variable dereference that occurs \\'\\'before\\'\\' the variable is assigned a value.  Another example would be to perform strong type checking when the language does not require such.  Thus, they are better at locating likely errors, versus actual errors.  As a result, these tools have a reputation of false positives.  The old Unix \\'\\'Lint programming tool|lint\\'\\' program is an early example.\\n\\nFor debugging electronic hardware (e.g., computer hardware) as well as low-level software (e.g., BIOSes, device drivers) and firmware, instruments such as oscilloscopes, logic analyzers or in-circuit emulator|in-circuit emulators (ICEs) are often used, alone or in combination.  An ICE may perform many of the typical software debugger\\'s tasks on low-level software and firmware.\\n\\nDebugging process \\nNormally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with Parallel computing|parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.\\n\\nAfter the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, a bug in a compiler can make it Crash (computing)|crash when parsing some large source file. However, after simplification of the test case, only few lines from the original source file can be sufficient to reproduce the same crash. Such simplification can be made manually, using a Divide and conquer algorithm|divide-and-conquer approach. The programmer will try to remove some parts of original test case and check if the problem still exists. When debugging the problem in a Graphical user interface|GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear.\\n\\nAfter the test case is sufficiently simplified, a programmer can use a debugger tool to examine program states (values of variables, plus the call stack) and track down the origin of the problem(s). Alternatively, Tracing (software)|tracing can be used. In simple cases, tracing is just a few print statements, which output the values of variables at certain points of program execution.{{citation needed|date=February 2016}}\\n\\n Techniques \\n \\'\\'Interactive debugging\\'\\'\\n \\'\\'{{visible anchor|Print debugging}}\\'\\' (or tracing) is the act of watching (live or recorded) trace statements, or print statements, that indicate the flow of execution of a process. This is sometimes called \\'\\'{{visible anchor|printf debugging}}\\'\\', due to the use of the printf function in C. This kind of debugging was turned on by the command TRON in the original versions of the novice-oriented BASIC programming language. TRON stood for, \"Trace On.\" TRON caused the line numbers of each BASIC command line to print as the program ran.\\n \\'\\'Remote debugging\\'\\' is the process of debugging a program running on a system different from the debugger. To start remote debugging, a debugger connects to a remote system over a network. The debugger can then control the execution of the program on the remote system and retrieve information about its state.\\n \\'\\'Post-mortem debugging\\'\\' is debugging of the program after it has already Crash (computing)|crashed. Related techniques often include various tracing techniques (for example,[http://www.drdobbs.com/tools/185300443 Postmortem Debugging, Stephen Wormuller, Dr. Dobbs Journal, 2006]) and/or analysis of memory dump (or core dump) of the crashed process. The dump of the process could be obtained automatically by the system (for example, when process has terminated due to an unhandled exception), or by a programmer-inserted instruction, or manually by the interactive user.\\n \\'\\'\"Wolf fence\" algorithm:\\'\\' Edward Gauss described this simple but very useful and now famous algorithm in a 1982 article for communications of the ACM as follows: \"There\\'s one wolf in Alaska; how do you find it? First build a fence down the middle of the state, wait for the wolf to howl, determine which side of the fence it is on. Repeat process on that side only, until you get to the point where you can see the wolf.\"<ref name=\"communications of the ACM\">{{cite journal | title=\"Pracniques: The \"Wolf Fence\" Algorithm for Debugging\", | author=E. J. Gauss | year=1982}} This is implemented e.g. in the Git (software)|Git version control system as the command \\'\\'git bisect\\'\\', which uses the above algorithm to determine which Commit (data management)|commit introduced a particular bug.\\n \\'\\'Delta Debugging\\'\\'{{snd}} a technique of automating test case simplification.Andreas Zeller: <cite>Why Programs Fail: A Guide to Systematic Debugging</cite>, Morgan Kaufmann, 2005. ISBN 1-55860-866-4{{rp|p.123}}<!-- for redirect from \\'Saff Squeeze\\' -->\\n \\'\\'Saff Squeeze\\'\\'{{snd}} a technique of isolating failure within the test using progressive inlining of parts of the failing test.[http://www.threeriversinstitute.org/HitEmHighHitEmLow.html Kent Beck, Hit \\'em High, Hit \\'em Low: Regression Testing and the Saff Squeeze]\\n\\nDebugging for embedded systems\\nIn contrast to the general purpose computer software design environment, a primary characteristic of embedded environments is the sheer number of different platforms available to the developers (CPU architectures, vendors, operating systems and their variants). Embedded systems are, by definition, not general-purpose designs: they are typically developed for a single task (or small range of tasks), and the platform is chosen specifically to optimize that application. Not only does this fact make life tough for embedded system developers, it also makes debugging and testing of these systems harder as well, since different debugging tools are needed in different platforms.\\n\\nto identify and fix bugs in the system (e.g. logical or synchronization problems in the code, or a design error in the hardware);\\nto collect information about the operating states of the system that may then be used to analyze the system: to find ways to boost its performance or to optimize other important characteristics (e.g. energy consumption, reliability, real-time response etc.).\\n\\nAnti-debugging\\nAnti-debugging is \"the implementation of one or more techniques within computer code that hinders attempts at reverse engineering or debugging a target process\".<ref name=\"veracode-antidebugging\">{{cite web |url=http://www.veracode.com/blog/2008/12/anti-debugging-series-part-i/ |title=Anti-Debugging Series - Part I |last=Shields |first=Tyler |date=2008-12-02 |work=Veracode |accessdate=2009-03-17}} It is actively used by recognized publishers in copy protection|copy-protection schemas, but is also used by malware to complicate its detection and elimination.<ref name=\"soft-prot\">[http://people.seas.harvard.edu/~mgagnon/software_protection_through_anti_debugging.pdf Software Protection through Anti-Debugging Michael N Gagnon, Stephen Taylor, Anup Ghosh] Techniques used in anti-debugging include:\\nAPI-based: check for the existence of a debugger using system information\\nException-based: check to see if exceptions are interfered with\\nProcess and thread blocks: check whether process and thread blocks have been manipulated\\nModified code: check for code modifications made by a debugger handling software breakpoints\\nHardware- and register-based: check for hardware breakpoints and CPU registers\\nTiming and latency: check the time taken for the execution of instructions\\nDetecting and penalizing debugger<ref name=\"soft-prot\" /><!-- reference does not exist -->\\n\\nAn early example of anti-debugging existed in early versions of Microsoft Word which, if a debugger was detected, produced a message that said: \"The tree of evil bears bitter fruit. Now trashing program disk.\", after which it caused the floppy disk drive to emit alarming noises with the intent of scaring the user away from attempting it again.<ref name=\"SecurityEngineeringRA\">{{cite book | url=http://www.cl.cam.ac.uk/~rja14/book.html | author=Ross J. Anderson | title=Security Engineering | isbn = 0-471-38922-6 | page=684 }}<ref name=\"toastytech\">{{cite web | url=http://toastytech.com/guis/word1153.html | title=Microsoft Word for DOS 1.15}}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "inner-jones",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", \"''\", 'Debugging', \"''\", \"'\", 'is', 'the', 'process', 'of', 'finding']\n",
      "[\"'\", \"''\", 'debugging', \"''\", \"'\", 'is', 'the', 'process', 'of', 'finding']\n",
      "[(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 66), ('to', 63), ('a', 60), ('``', 47), ('in', 44), ('and', 41)]\n"
     ]
    }
   ],
   "source": [
    "# 步驟一：Tokenize the article\n",
    "tokens = word_tokenize(article)\n",
    "print(tokens[:10])\n",
    "\n",
    "# 步驟二：將所有大寫轉成小寫\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "print(lower_tokens[:10])\n",
    "\n",
    "# 步驟三：計算頻率\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# 步驟四：印出最常出現的10個tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-madrid",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-emergency",
   "metadata": {},
   "source": [
    "# Simple Text Preprocessing\n",
    "\n",
    "用處： <br/>\n",
    "產生較好的input data，對於未來的machine learning methods有幫助。例如：將所有的tokens變成小寫，結果一定比有大寫小寫之分的結果好\n",
    "\n",
    "---\n",
    "\n",
    "## 其他preprocessing techniques：\n",
    "\n",
    "### Stemming and lemmatization 將字詞還原\n",
    "\n",
    "Stemming: 將字詞還原成 **root forms**\n",
    "\n",
    "例如： <br/>\n",
    "stays, staying, stayed --> stay <br/>\n",
    "house, housing, houses --> hous <br/>\n",
    "事實上，stemming 會將 '-ed', '-ing', '-er' 以及負數 '-s' 和所有格的字尾去除，就算去除之後的字詞不是一個正確的英文字也會去除。\n",
    "\n",
    "可以透過 nltk 的 **PorterStemmer library** 實踐 (相當快)，也可以用 **SnowballStemmer** 來實現多種語言的 stemming\n",
    "\n",
    "---\n",
    "\n",
    "Lemmatization: 跟 stemming 很像，只是 lemmatization 後得到的原型字詞都是正確存在的英文字詞\n",
    "\n",
    "例如： <br/>\n",
    "stay, stays, staying, stayed --> stay <br/>\n",
    "house, housing, houses --> house <br/>\n",
    "\n",
    "可以透過 nltk 的 WordNetLemmatizer 實踐\n",
    "\n",
    "---\n",
    "\n",
    "Lemmatization vs Stemming\n",
    "\n",
    "1. Stemming 的結果並不都是正確的字詞，因此如果要產生 actual words，要用 Lemmatization\n",
    "2. Stemming 較為迅速，因為是直接刪掉字尾\n",
    "3. Lemmatization 跟一個字詞的 part of speech 有關，要根據該字詞的 part of speech 詞性來做還原\n",
    "\n",
    "### Stop words、Punctuations\n",
    "\n",
    "移除stopwords，刪除一些大量出現卻無意義的字詞 (例如：and, the, is) <br/>\n",
    "移除標點符號或其他不構成文意解讀的tokens\n",
    "\n",
    "經過不同的preprocessing processes都會得到不同的分析結果，因此建議可以測試多種結果之後再選一個最好的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-currency",
   "metadata": {},
   "source": [
    "---\n",
    "#### 例子一\n",
    "輸入字串為：Cats, dogs and birds are common pets. So are fish. <br/>\n",
    "輸出tokens：cat, dog, bird, common, pet, fish\n",
    "\n",
    "可以看出 (1) 大寫變成小寫 (2) stopwords移除 (3) 複數變成單數 (4) 標點符號移除 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "seven-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-trial",
   "metadata": {},
   "source": [
    "stopwords是一個裝有各語言的stopwords的物件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assumed-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The cat is in the box. The cat likes the box. The box is over the cat.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-terrace",
   "metadata": {},
   "source": [
    "步驟一：產生tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "technical-oxford",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'cat',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'box',\n",
       " '.',\n",
       " 'The',\n",
       " 'cat',\n",
       " 'likes',\n",
       " 'the',\n",
       " 'box',\n",
       " '.',\n",
       " 'The',\n",
       " 'box',\n",
       " 'is',\n",
       " 'over',\n",
       " 'the',\n",
       " 'cat',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tokens = word_tokenize(text)\n",
    "raw_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-tanzania",
   "metadata": {},
   "source": [
    "步驟二：消除non-alphabetic tokens (包含標點、特殊符號與數字) 並全部變小寫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "statistical-bracket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'cat',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'box',\n",
       " 'the',\n",
       " 'cat',\n",
       " 'likes',\n",
       " 'the',\n",
       " 'box',\n",
       " 'the',\n",
       " 'box',\n",
       " 'is',\n",
       " 'over',\n",
       " 'the',\n",
       " 'cat']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_tokens = [w.lower() for w in raw_tokens if w.isalpha()]\n",
    "alpha_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-special",
   "metadata": {},
   "source": [
    "步驟三：移除stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "intimate-support",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'box', 'cat', 'likes', 'box', 'box', 'cat']\n"
     ]
    }
   ],
   "source": [
    "final_tokens = [w for w in lower_tokens if w not in stopwords.words('english')]\n",
    "print(final_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-fortune",
   "metadata": {},
   "source": [
    "步驟四：count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "quality-brand",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('box', 3)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(final_tokens).most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-dayton",
   "metadata": {},
   "source": [
    "---\n",
    "延續課堂練習一 (tokens已經被轉成小寫了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cooperative-toner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\", \"''\", 'debugging', \"''\", \"'\", 'is', 'the', 'process', 'of', 'finding']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 檢視lower_tokens\n",
    "lower_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "swedish-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-bonus",
   "metadata": {},
   "source": [
    "移除non-alphabetic tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "impossible-soldier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['debugging',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'finding',\n",
       " 'and',\n",
       " 'resolving',\n",
       " 'of',\n",
       " 'defects']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "alpha_only[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-overview",
   "metadata": {},
   "source": [
    "移除 English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "pressing-samoa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['debugging',\n",
       " 'process',\n",
       " 'finding',\n",
       " 'resolving',\n",
       " 'defects',\n",
       " 'prevent',\n",
       " 'correct',\n",
       " 'operation',\n",
       " 'computer',\n",
       " 'software']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "no_stops[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-lodging",
   "metadata": {},
   "source": [
    "將tokens全部還原成原形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "pretty-cisco",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to D:/Programming\n",
      "[nltk_data]     Exercises/Datacamp/Python/Introduction to Natural\n",
      "[nltk_data]     Language Processing in Python/nltk_wordnet...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 下載用來lemmatize所有tokens的corpus\n",
    "# nltk.download(\"wordnet\", \"D:/Programming Exercises/Datacamp/Python/Introduction to Natural Language Processing in Python/Corpus/nltk_wordnet\")\n",
    "\n",
    "# 告知python我用來lemmatize的corpus的路徑\n",
    "nltk.data.path.append('./Corpus/nltk_wordnet/')\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-silver",
   "metadata": {},
   "source": [
    "計算bag-of-words數量並印出頻率最高的前10名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "central-invite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('debugging', 40), ('system', 25), ('bug', 17), ('software', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('debugger', 13)]\n"
     ]
    }
   ],
   "source": [
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-newman",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-intention",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "\n",
    "Gensim也是一個很廣泛使用的open-source natural language processing library。\n",
    "\n",
    "Gensim使用top academic models來完成複雜性高的tasks，例如：\n",
    "1. building document vectors或word vectors (將tokens數字化)\n",
    "2. Topic identification或document comparison (比較document vectors或word vectors的相似度)\n",
    "\n",
    "Gensim可用簡單的函數就建立corpora(語料庫，corpus的複數) <br/>\n",
    "Corpus由一堆的texts組成，用來完成各種NLP的tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-syndication",
   "metadata": {},
   "source": [
    "## Word Vectors\n",
    "\n",
    "![](Image/Image7.jpg)\n",
    "\n",
    "透過將文字轉換成向量，再用一些topic modelling algorithms，達成比較相似度的結果\n",
    "\n",
    "例如：LDA、LSA、PLSA等等演算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-leather",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "舉例：將一堆文件組成的文件集tokenize後，進行編碼，並得到各個document各自的bag-of-words (corpus)\n",
    "\n",
    "做出來的corpus可以用來產生Tf-idf的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "plastic-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-passage",
   "metadata": {},
   "source": [
    "Initialize our documents (my_documents裡面包含6個documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ongoing-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_documents = ['The movie is about a spaceship and aliens.',\n",
    "               'I really like the movie!',\n",
    "               'Awesome action scenes, but boring characters.',\n",
    "               'The movie is awful! I hate alien films.',\n",
    "               'Space is cool! I like the movie.',\n",
    "               'More space films, please!']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-notion",
   "metadata": {},
   "source": [
    "Tokenize並且轉小寫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "discrete-installation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'movie', 'is', 'about', 'a', 'spaceship', 'and', 'aliens', '.'],\n",
       " ['i', 'really', 'like', 'the', 'movie', '!'],\n",
       " ['awesome', 'action', 'scenes', ',', 'but', 'boring', 'characters', '.'],\n",
       " ['the', 'movie', 'is', 'awful', '!', 'i', 'hate', 'alien', 'films', '.'],\n",
       " ['space', 'is', 'cool', '!', 'i', 'like', 'the', 'movie', '.'],\n",
       " ['more', 'space', 'films', ',', 'please', '!']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-surfing",
   "metadata": {},
   "source": [
    "將tokens全部編碼成數字id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "purple-wildlife",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x21f2eb7c4c8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_docs)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-engineer",
   "metadata": {},
   "source": [
    "印出各個tokens的編碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "naked-coverage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'about': 2,\n",
       " 'aliens': 3,\n",
       " 'and': 4,\n",
       " 'is': 5,\n",
       " 'movie': 6,\n",
       " 'spaceship': 7,\n",
       " 'the': 8,\n",
       " '!': 9,\n",
       " 'i': 10,\n",
       " 'like': 11,\n",
       " 'really': 12,\n",
       " ',': 13,\n",
       " 'action': 14,\n",
       " 'awesome': 15,\n",
       " 'boring': 16,\n",
       " 'but': 17,\n",
       " 'characters': 18,\n",
       " 'scenes': 19,\n",
       " 'alien': 20,\n",
       " 'awful': 21,\n",
       " 'films': 22,\n",
       " 'hate': 23,\n",
       " 'cool': 24,\n",
       " 'space': 25,\n",
       " 'more': 26,\n",
       " 'please': 27}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-irish",
   "metadata": {},
   "source": [
    "用我們得到的dictionary來建立一個gensim corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "certain-shower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(6, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
       " [(0, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1)],\n",
       " [(0, 1), (5, 1), (6, 1), (8, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1)],\n",
       " [(9, 1), (13, 1), (22, 1), (25, 1), (26, 1), (27, 1)]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-printing",
   "metadata": {},
   "source": [
    "Gensim corpus和一般的corpus很不一樣，一般的corpus只是一些documents的集合。\n",
    "\n",
    "Gensim 用簡單的bag-of-words模型將每個document轉換成**token id**和其**出現頻率**的pairs，也就是說，將原本document中的所有字詞都用token id來編碼，然後計算每個相同的編碼各出現幾次\n",
    "\n",
    "譬如說，上面的例子，輸出的Gensim corpus是由**6個lists**組成，分別代表**6個documents**。同時，每個document的list是由**數個tuple**組成，每個tuple代表該document的一個**token**。tuple的第一個element代表token的id，第二個element代表出現頻率。\n",
    "\n",
    "一個corpus中，就算位於**不同的document*，**同樣的token**還是會有**一樣的token id**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-calgary",
   "metadata": {},
   "source": [
    "#### Gensim models可以簡單的儲存、更新並使用\n",
    "\n",
    "---\n",
    "\n",
    "Get top 3 words within each document along side the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "compact-winning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document1\n",
      ". 1\n",
      "a 1\n",
      "about 1\n",
      "document2\n",
      "movie 1\n",
      "the 1\n",
      "! 1\n",
      "document3\n",
      ". 1\n",
      ", 1\n",
      "action 1\n",
      "document4\n",
      ". 1\n",
      "is 1\n",
      "movie 1\n",
      "document5\n",
      ". 1\n",
      "is 1\n",
      "movie 1\n",
      "document6\n",
      "! 1\n",
      ", 1\n",
      "films 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    bow_doc = sorted(corpus[i], key = lambda x: x[1], reverse = True)\n",
    "    print(\"document\" + str(i+1))\n",
    "    for word_id, word_count in bow_doc[:3]:\n",
    "        print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-entrepreneur",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-studio",
   "metadata": {},
   "source": [
    "# Tf-idf\n",
    "\n",
    "\n",
    "這個方法可以用來找出每個document最重要的字詞 (不只看詞頻，也看逆向文件頻率)，透過數學的計算移除Stopwords這種頻繁出現卻沒意義的字詞的重要性。\n",
    "\n",
    "此外，除了stopwords以外，也有一些字詞雖然出現頻率高，但實際重要性不高。以關於astronomy的文件來說，sky可能很常出現在該文件中，但是也是有可能出現在其他文件，這樣他對於該文件的重要性會因為出現在其他文件而降低。\n",
    "\n",
    "TF-IDF 能使document specific frequent words的重要性高，而common words across the entire corpus的重要性低。\n",
    "\n",
    "---\n",
    "\n",
    "### 公式\n",
    "\n",
    "![](Image/Image8.jpg)\n",
    "\n",
    "公式的左邊那一項代表一個字詞在一個document中的出現頻率 (次數除以文件總字數)。這個很直覺，因為出現次數多，重要性應該較高 <br/>\n",
    "公式的右邊那一項代表該字詞在整個corpus的common程度。如果幾乎每個文件都有出現，則右邊那項很小；如果只在少數幾個文件中出現，則右邊那項很大\n",
    "\n",
    "也就是說，一些stopwords，在每個文件都有出現，因此$N = df$，這樣 $log(N/df) = 0$，毫無重要性。 <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-indie",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Tf-idf in Gensim\n",
    "\n",
    "利用前面做出來的gensim corpus (由各個document裡面的token id和token count組成的gensim corpus，而不是一般只是包住一堆文件的corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "hungarian-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# initialize a tfidf model\n",
    "tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-territory",
   "metadata": {},
   "source": [
    "檢視每個document中各token的tf-idf值的大小，可以推斷某些tokens是不是比較重要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "administrative-accessory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document1\n",
      "[('.', 0.09826557421394637), ('a', 0.4342377915536977), ('about', 0.4342377915536977), ('aliens', 0.4342377915536977), ('and', 0.4342377915536977), ('is', 0.16798610866987568), ('movie', 0.09826557421394637), ('spaceship', 0.4342377915536977), ('the', 0.09826557421394637)]\n",
      "Document2\n",
      "[('movie', 0.1746298276735174), ('the', 0.1746298276735174), ('!', 0.1746298276735174), ('i', 0.29853166221463673), ('like', 0.47316148988815415), ('really', 0.7716931521027908)]\n",
      "Document3\n",
      "[('.', 0.08926151827345048), (',', 0.2418550916450883), ('action', 0.3944486650167261), ('awesome', 0.3944486650167261), ('boring', 0.3944486650167261), ('but', 0.3944486650167261), ('characters', 0.3944486650167261), ('scenes', 0.3944486650167261)]\n",
      "Document4\n",
      "[('.', 0.1148821431389833), ('is', 0.196392320870746), ('movie', 0.1148821431389833), ('the', 0.1148821431389833), ('!', 0.1148821431389833), ('i', 0.196392320870746), ('alien', 0.5076667848804753), ('awful', 0.5076667848804753), ('films', 0.31127446400972936), ('hate', 0.5076667848804753)]\n",
      "Document5\n",
      "[('.', 0.15066064707629043), ('is', 0.257556077339235), ('movie', 0.15066064707629043), ('the', 0.15066064707629043), ('!', 0.15066064707629043), ('i', 0.257556077339235), ('like', 0.4082167244155255), ('cool', 0.6657728017547605), ('space', 0.4082167244155255)]\n",
      "Document6\n",
      "[('!', 0.1269183979212522), (',', 0.34388683224786987), ('films', 0.34388683224786987), ('space', 0.34388683224786987), ('more', 0.5608552665744875), ('please', 0.5608552665744875)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(\"Document\" + str(i+1))\n",
    "    print([(dictionary.get(token[0]), token[1]) for token in tfidf[corpus[i]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-macro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Implement tf-idf with the Wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wiki_df = pd.read_csv(\"Datasets/Wikipedia articles/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
