{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "still-enclosure",
   "metadata": {},
   "source": [
    "# Classifying fake news using supervised learning with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-defensive",
   "metadata": {},
   "source": [
    "使用Scikit learn套件來做supervised learning\n",
    "\n",
    "我們使用bag-of-words models或TF-IDF models作為features，來建立supervised learning data from text\n",
    "\n",
    "![](Image/Image11.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-omega",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1. 載入所需套件：Pandas、sklearn的train_test_split、sklearn的CountVectorizer\n",
    "2. Load data into DataFrame\n",
    "3. 將資料分成training set和testing set\n",
    "4. 建立一個sklearn的CountVectorizer模型，並傳入stop_words = 'english'這個參數 (此模型很像Gensim，能將我的text轉換成bag-of-word vectors，並能移除stopwords)\n",
    "5. 呼叫CountVectorizer物件的fit_transform方法來實際將training set轉換成word vectors的dictionary (產生word的id對應該字在一句話出現的次數)\n",
    "6. CountVectorizer模型處理過後得到的每個token，都是這個classification problem的feature\n",
    "7. 由於training data的features被transformed成word vectors了，因此testing data也要做相同的轉換\n",
    "\n",
    "範例：\n",
    "\n",
    "![](Image/Image12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-aging",
   "metadata": {},
   "source": [
    "### Import dataset \n",
    "\n",
    "此資料有四個 columns：\n",
    "1. Identifier\n",
    "2. Title (新聞標題)\n",
    "3. Text (新聞內文)\n",
    "4. Label (是否為假新聞 FAKE or REAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "twenty-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"Datasets/fake_or_real_news.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-drink",
   "metadata": {},
   "source": [
    "課堂練習一：實作CountVectorizer (結果為整數，因為回傳詞出現的次數)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "domestic-silver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              title  \\\n",
      "0        8476                       You Can Smell Hillary’s Fear   \n",
      "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
      "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
      "4         875   The Battle of New York: Why This Primary Matters   \n",
      "\n",
      "                                                text label  \n",
      "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
      "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
      "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
      "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
      "4  It's primary day in New York and front-runners...  REAL  \n",
      "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], y, test_size = 0.33, random_state = 53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-instrument",
   "metadata": {},
   "source": [
    "get_feature_names()這個方法可以得到轉換過後的column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-investigation",
   "metadata": {},
   "source": [
    "課堂練習二：嘗試另一個sklearn的模型 TfidfVectorizer (結果為小數，因為tfidf代表token的詞頻率)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "handmade-locator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-prairie",
   "metadata": {},
   "source": [
    "上面例子的A代表attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-aside",
   "metadata": {},
   "source": [
    "課堂練習三：比較 CountVectorizer 和 TfidfVectorizer 所建立的features的差異"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "biological-prayer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "1   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "2   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "3   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "4   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "\n",
      "   حلب  عربي  عن  لم  ما  محاولات  من  هذا  والمرضى  ยงade  \n",
      "0    0     0   0   0   0        0   0    0        0      0  \n",
      "1    0     0   0   0   0        0   0    0        0      0  \n",
      "2    0     0   0   0   0        0   0    0        0      0  \n",
      "3    0     0   0   0   0        0   0    0        0      0  \n",
      "4    0     0   0   0   0        0   0    0        0      0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "    00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "1  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "2  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "3  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "4  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "\n",
      "   حلب  عربي   عن   لم   ما  محاولات   من  هذا  والمرضى  ยงade  \n",
      "0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "1  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "2  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "3  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "4  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "set()\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-regard",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-tamil",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Naive Bayes Classifier\n",
    "\n",
    "使用條件機率模型來判斷一個特定的輸入的結果\n",
    "\n",
    "It is a simple and effective tool to build a fake news classifier.\n",
    "\n",
    "例子：Given a particular piece of data, how likely is a particular outcome? <br/>\n",
    "有一串電影的描述，根據內容出現的字詞來判斷屬於哪種電影 <br/>\n",
    "有一串新聞，根據內容出現的文字判斷是否為假新聞 <br/>\n",
    "\n",
    "### Naive Bayes Classifier in Sklearn\n",
    "\n",
    "Multinomial Naive Bayes 模型很適合處理 CountVectorize 出來的 word vectors 結果，因為 MultinomialNB 的 input 要是 integer\n",
    "\n",
    "因此 Multinomial Naive Bayes 模型處理 float 類型的 features (像是 TfidfVectorizer 的結果) 可能不會得到很好的結果。因此如果我用 TfidfVectorizer 來將文字轉換成 word vectors 字典，可以先測試 Multinomial Naive Bayes 的結果；如果結果不理想，再用 SVM 或 linear models 來建立分類模型。\n",
    "\n",
    "---\n",
    "\n",
    "### 實作：用 Multinomial Naive Bayes 來 model fake news classifier (CountVectorizer的結果為整數)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "consecutive-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library and function\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Evaluate model performance\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "parliamentary-eight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.893352462936394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 865,  143],\n",
       "       [  80, 1003]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化 naive bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# fit模型\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# 預測 (分類) \n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# 評估結果好壞 (accuracy)\n",
    "print(\"Accuracy: \"+str(metrics.accuracy_score(y_test, pred)))    # 先傳入真實值，再傳入預測結果\n",
    "\n",
    "# 評估模型好壞 (confusion matrix)\n",
    "metrics.confusion_matrix(y_test, pred, labels = ['FAKE', \"REAL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-madonna",
   "metadata": {},
   "source": [
    "### 用 MultinomialNB 來 fit TfidfVectorizer 的結果 (小數)\n",
    "\n",
    "測試看看 **TfidfVectorizer** 的結果是否適合用 MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "taken-worst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8565279770444764\n",
      "[[ 739  269]\n",
      " [  31 1052]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy: \"+str(score))\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels = [\"FAKE\",\"REAL\"])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-saudi",
   "metadata": {},
   "source": [
    "### 測試不同的 alpha 值\n",
    "\n",
    "alpha 為 MultinomialNB 的一個參數，為一個介於 0 到 1 的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "funky-pittsburgh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.8813964610234337\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.8976566236250598\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.8938307030129125\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.8900047824007652\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.8857006217120995\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.8842659014825442\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.874701099952176\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.8703969392635102\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.8660927785748446\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.8589191774270684\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\naive_bayes.py:509: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0, 1, 0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha = alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-johns",
   "metadata": {},
   "source": [
    "### Inspecting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "inner-discount",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE [(-11.316312804238807, '0000'), (-11.316312804238807, '000035'), (-11.316312804238807, '0001'), (-11.316312804238807, '0001pt'), (-11.316312804238807, '000km'), (-11.316312804238807, '0011'), (-11.316312804238807, '006s'), (-11.316312804238807, '007'), (-11.316312804238807, '007s'), (-11.316312804238807, '008s'), (-11.316312804238807, '0099'), (-11.316312804238807, '00am'), (-11.316312804238807, '00p'), (-11.316312804238807, '00pm'), (-11.316312804238807, '014'), (-11.316312804238807, '015'), (-11.316312804238807, '018'), (-11.316312804238807, '01am'), (-11.316312804238807, '020'), (-11.316312804238807, '023')]\n",
      "REAL [(-7.742481952533027, 'states'), (-7.717550034444668, 'rubio'), (-7.703583809227384, 'voters'), (-7.654774992495461, 'house'), (-7.649398936153309, 'republicans'), (-7.6246184189367, 'bush'), (-7.616556675728881, 'percent'), (-7.545789237823644, 'people'), (-7.516447881078008, 'new'), (-7.448027933291952, 'party'), (-7.411148410203476, 'cruz'), (-7.410910239085596, 'state'), (-7.35748985914622, 'republican'), (-7.33649923948987, 'campaign'), (-7.2854057032685775, 'president'), (-7.2166878130917755, 'sanders'), (-7.108263114902301, 'obama'), (-6.724771332488041, 'clinton'), (-6.5653954389926845, 'said'), (-6.328486029596207, 'trump')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\deprecation.py:101: FutureWarning: Attribute coef_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-edmonton",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-jonathan",
   "metadata": {},
   "source": [
    "# NLP 的各種問題\n",
    "\n",
    "1. Translation\n",
    "2. Sentiment Analysis\n",
    "3. Language Biases (如果用有biases的word vectors來訓練模型，則模型可能也會有偏見)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
